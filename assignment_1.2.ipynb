{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import requests\n",
    "import json\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "test_folder = \"/content/drive/MyDrive/htx-sample/pdf\"\n",
    "\n",
    "# all pdfs \n",
    "pdf_files = [f for f in os.listdir(test_folder) if f.lower().endswith('.pdf')]\n",
    "\n",
    "output_path_dir = \"/content/drive/MyDrive/htx-sample/md\"\n",
    "\n",
    "converter = DocumentConverter()\n",
    "\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_path = os.path.join(test_folder, pdf_file)\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing: {pdf_file}\")\n",
    "        result = converter.convert(pdf_path)\n",
    "        \n",
    "        markdown_content = result.document.export_to_markdown()\n",
    "        \n",
    "        output_filename = pdf_file.replace('.pdf', '.md')\n",
    "        output_path = os.path.join(output_path_dir, output_filename)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(markdown_content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {pdf_file}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291ed399",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_folder = \"/content/drive/MyDrive/htx-sample/md\"\n",
    "\n",
    "output_path_headers = \"/content/drive/MyDrive/htx-sample/chunks/chunks_header_based.jsonl\"\n",
    "\n",
    "output_path_bert = \"/content/drive/MyDrive/htx-sample/chunks/chunks_bert_based.jsonl\"\n",
    "\n",
    "md_files = [f for f in os.listdir(md_folder) if f.lower().endswith('.md')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b8ffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Header Chunking \n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "md_files = [f for f in os.listdir(md_folder) if f.lower().endswith('.md')]\n",
    "all_chunks = []\n",
    "\n",
    "for md_file in md_files:\n",
    "    with open(os.path.join(md_folder, md_file), 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    lines = text.split('\\n')\n",
    "    current_chunk = \"\"\n",
    "    chunk_id = 1\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip().startswith('#'):\n",
    "            if current_chunk.strip():\n",
    "                all_chunks.append({\n",
    "                    \"source_file\": md_file,\n",
    "                    \"chunk_id\": chunk_id,\n",
    "                    \"chunk\": current_chunk.strip(),\n",
    "                    \"chunk_size\": len(current_chunk.strip())\n",
    "                })\n",
    "                chunk_id += 1\n",
    "            current_chunk = line + '\\n'\n",
    "        else:\n",
    "            current_chunk += line + '\\n'\n",
    "    \n",
    "    if current_chunk.strip():\n",
    "        all_chunks.append({\n",
    "            \"source_file\": md_file,\n",
    "            \"chunk_id\": chunk_id,\n",
    "            \"chunk\": current_chunk.strip(),\n",
    "            \"chunk_size\": len(current_chunk.strip())\n",
    "        })\n",
    "\n",
    "with open(output_path_headers, 'w', encoding='utf-8') as f:\n",
    "    for chunk in all_chunks:\n",
    "        json.dump(chunk, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"total chunks: {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe36eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Chunk Grouping \n",
    "\n",
    "Adding of contexutla information \n",
    "\"\"\"\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "def group_chunks_by_tokens(chunks, min_tokens=512):\n",
    "    grouped_chunks = []\n",
    "    current_group = []\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        chunk_tokens = count_tokens(chunk['chunk'])\n",
    "        \n",
    "        if current_tokens + chunk_tokens >= min_tokens and current_group:\n",
    "            grouped_chunks.append({\n",
    "                'chunks': current_group,\n",
    "                'total_tokens': current_tokens,\n",
    "                'combined_text': '\\n\\n'.join([c['chunk'] for c in current_group])\n",
    "            })\n",
    "            current_group = [chunk]\n",
    "            current_tokens = chunk_tokens\n",
    "        else:\n",
    "            current_group.append(chunk)\n",
    "            current_tokens += chunk_tokens\n",
    "    \n",
    "    if current_group:\n",
    "        grouped_chunks.append({\n",
    "            'chunks': current_group,\n",
    "            'total_tokens': current_tokens,\n",
    "            'combined_text': '\\n\\n'.join([c['chunk'] for c in current_group])\n",
    "        })\n",
    "    \n",
    "    return grouped_chunks\n",
    "\n",
    "def add_contextual_information(grouped_chunks):\n",
    "    \"\"\"\n",
    "    Chunking Strategy\n",
    "\n",
    "\n",
    "    1. Given the raw chunks seperated by headers, combine each base chunk to form a chunk size = MIN_CHUNK_TOKENS \n",
    "\n",
    "    2. Following anthropic Introducing Contextual Retrieval -> Append contextual information to each chunk by passing Chunk + Document to a LM\n",
    "\n",
    "    3. Grounds each chunk with contexual infromation, such that when used for downstream generation, its able to reference the context of the whole document, \n",
    "       rather than a portion of the document instead \n",
    "\n",
    "    \"\"\"\n",
    "    enhanced_chunks = []\n",
    "    \n",
    "    for i, group in enumerate(grouped_chunks):\n",
    "        combined_text = group['combined_text']\n",
    "        \n",
    "        prompt = f\"\"\"<document_context>\n",
    "            You are analyzing a document chunk and need to add helpful contextual information that would make this chunk more useful for retrieval and understanding when used in isolation.\n",
    "\n",
    "            Add contextual information such as:\n",
    "            - What this section is about (brief summary)\n",
    "            - Key concepts or entities mentioned\n",
    "            - How this relates to the broader document topic\n",
    "            - Any important background needed to understand this content\n",
    "\n",
    "            Keep the contextual information concise but informative.\n",
    "            </document_context>\n",
    "\n",
    "            Original chunk:\n",
    "            {combined_text}\n",
    "\n",
    "            Please provide:\n",
    "            1. A brief contextual summary (2-3 sentences)\n",
    "            2. Key concepts/entities mentioned\n",
    "            3. The enhanced chunk with contextual information added\n",
    "\n",
    "            Format your response as:\n",
    "            CONTEXTUAL_SUMMARY: [your summary]\n",
    "            KEY_CONCEPTS: [comma-separated list]\n",
    "            ENHANCED_CHUNK: [original content with contextual information seamlessly integrated]\n",
    "      \"\"\"\n",
    "\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer .\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": \"qwen/qwen3-235b-a22b-2507\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 2000\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                headers=headers,\n",
    "                json=data\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                enhanced_content = result['choices'][0]['message']['content']\n",
    "                \n",
    "                contextual_summary = \"\"\n",
    "                key_concepts = \"\"\n",
    "                enhanced_chunk = combined_text\n",
    "                \n",
    "                lines = enhanced_content.split('\\n')\n",
    "                for line in lines:\n",
    "                    if line.startswith('CONTEXTUAL_SUMMARY:'):\n",
    "                        contextual_summary = line.replace('CONTEXTUAL_SUMMARY:', '').strip()\n",
    "                    elif line.startswith('KEY_CONCEPTS:'):\n",
    "                        key_concepts = line.replace('KEY_CONCEPTS:', '').strip()\n",
    "                    elif line.startswith('ENHANCED_CHUNK:'):\n",
    "                        enhanced_chunk = enhanced_content[enhanced_content.find('ENHANCED_CHUNK:') + len('ENHANCED_CHUNK:'):].strip()\n",
    "                \n",
    "                enhanced_chunks.append({\n",
    "                    'group_id': i + 1,\n",
    "                    'original_chunks': group['chunks'],\n",
    "                    'total_tokens': group['total_tokens'],\n",
    "                    'contextual_summary': contextual_summary,\n",
    "                    'key_concepts': key_concepts,\n",
    "                    'enhanced_content': enhanced_chunk,\n",
    "                    'original_content': combined_text\n",
    "                })\n",
    "                \n",
    "                print(f\"{i+1}/{len(grouped_chunks)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"error calling api\")\n",
    "            continue \n",
    "\n",
    "    return enhanced_chunks\n",
    "\n",
    "min_token_threshold = 512\n",
    "\n",
    "grouped_chunks = group_chunks_by_tokens(all_chunks, min_tokens=min_token_threshold)\n",
    "print(f\"{len(grouped_chunks)} grouped chunks\")\n",
    "\n",
    "enhanced_chunks = add_contextual_information(grouped_chunks)\n",
    "\n",
    "with open('enhanced_chunks.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for chunk in enhanced_chunks:\n",
    "        json.dump(chunk, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"{len(enhanced_chunks)} chunks in total\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
